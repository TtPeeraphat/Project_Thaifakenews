{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c3c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import coalesce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad5382",
   "metadata": {},
   "source": [
    "pip install --quiet transformers\n",
    "pip install torch\n",
    "pip install pandas\n",
    "pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7bab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# โหลดไฟล์ CSV (กรณีชื่อคอลัมน์ 'label')\n",
    "df = pd.read_csv('news_dataset_truefakenews.csv')\n",
    "# ถ้าชื่อคอลัมน์สะกดผิด\n",
    "df = df.rename(columns={'lable':'label', 'autor':'author'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de19749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# กำหนด mapping dictionary\n",
    "label_map = {\n",
    "    'ข่าวจริง': 'true',\n",
    "    'คลังความรู้': 'true',\n",
    "    'ข่าวปลอม': 'fake',\n",
    "    'อาชญากรออนไลน์': 'fake',\n",
    "    'ข่าวบิดเบือน': 'fake',\n",
    "    'ข่าวอื่นๆ': None  # หรือจะลบออกเลย\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b381a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ทำความสะอาด label\n",
    "ZW = ''.join(['\\u200B','\\u200C','\\u200D','\\uFEFF'])\n",
    "\n",
    "def normalize_thai(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).replace('\\u00A0',' ').translate({ord(ch):None for ch in ZW})\n",
    "    s = unicodedata.normalize('NFC', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r'([\\u0E31\\u0E34-\\u0E3A\\u0E47-\\u0E4E])\\1+', r'\\1', s)\n",
    "    return s\n",
    "\n",
    "df['label'] = df['label'].apply(normalize_thai)\n",
    "\n",
    "# เลือกฉลากมาตรฐาน\n",
    "CANON = {'ข่าวจริง':'ข่าวจริง', 'ข่าวปลอม':'ข่าวปลอม'}\n",
    "\n",
    "# map เคสเพี้ยน\n",
    "ALIASES = {\n",
    "    'ข่าวจริิง':'ข่าวจริง',\n",
    "    'ข่่าวจริง':'ข่าวจริง',\n",
    "    'ข่่าวบิดเบือน':'ข่าวบิดเบือน',\n",
    "}\n",
    "df['label'] = df['label'].replace(ALIASES)\n",
    "\n",
    "# กรองฉลากที่ต้องการและ drop row ที่ไม่มี title/content\n",
    "df = df[df['label'].isin(CANON.keys())].dropna(subset=['title','content']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03853ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date                                         link  \\\n",
      "0  23/07/2568 09:00:20  https://www.antifakenewscenter.com/?p=74581   \n",
      "1  23/07/2568 08:00:51  https://www.antifakenewscenter.com/?p=74577   \n",
      "2  23/07/2568 07:00:51  https://www.antifakenewscenter.com/?p=74573   \n",
      "3  23/07/2568 06:30:23  https://www.antifakenewscenter.com/?p=74569   \n",
      "4  23/07/2568 03:30:03  https://www.antifakenewscenter.com/?p=74561   \n",
      "\n",
      "                                               title  \\\n",
      "0  OR ชวนร่วมเป็นเจ้าของกิจการ ลงทุนหุ้น IPO เริ่...   \n",
      "1  กระทรวงยุติธรรม เปิดเพจเฟซบุ๊กชื่อ Technology ...   \n",
      "2  หุ้น OKJ เปิดเทรดวันแรก ราคาเปิดพอร์ตเริ่มต้น ...   \n",
      "3  กรุงไทย เปิดให้จองสิทธิ์ยืมเงิน 100,000 บาท ลง...   \n",
      "4            ก.ล.ต. เปิดเว็บไซต์ให้ผู้ลงทุนสะสมคะแนน   \n",
      "\n",
      "                                             content  \\\n",
      "0  จากที่มีการโฆษณาระบุเปิดให้ลงทุนหุ้น IPO ข้างต...   \n",
      "1  ระวังเพจปลอมแอบอ้างหน่วยงาน เพจเฟซบุ๊กชื่อ Tec...   \n",
      "2  เช็กข้อมูลให้ชัวร์ก่อนติดตาม เพจ “OH KAD” เป็น...   \n",
      "3  การปล่อยสินเชื่อ บนบัญชี TikTok ชื่อ ktb.thai ...   \n",
      "4  ดูให้ดี เช็กให้ชัวร์ เนื่องจากมิจฉาชีพได้นำโลโ...   \n",
      "\n",
      "                                              author     label  \\\n",
      "0  บริษัท ปตท. น้ำมันและการค้าปลีก จำกัด (มหาชน) ...  ข่าวปลอม   \n",
      "1        สำนักงานปลัดกระทรวงยุติธรรม กระทรวงยุติธรรม  ข่าวปลอม   \n",
      "2                        ตลาดหลักทรัพย์แห่งประเทศไทย  ข่าวปลอม   \n",
      "3                       ธนาคารกรุงไทย กระทรวงการคลัง  ข่าวปลอม   \n",
      "4  สำนักงานคณะกรรมการกำกับหลักทรัพย์และตลาดหลักทร...  ข่าวปลอม   \n",
      "\n",
      "               category label_binary  \n",
      "0          การเงิน-หุ้น         fake  \n",
      "1  นโยบายรัฐบาล-ข่าวสาร         fake  \n",
      "2          การเงิน-หุ้น         fake  \n",
      "3          การเงิน-หุ้น         fake  \n",
      "4          การเงิน-หุ้น         fake  \n"
     ]
    }
   ],
   "source": [
    "# สร้างคอลัมน์ใหม่ 'label_binary' เป็น binary label จาก mapping\n",
    "#df['label_binary'] = df['label'].map(label_map)\n",
    "\n",
    "# ลบแถวที่เป็น None (ข่าวอื่นๆ)\n",
    "#df = df.dropna(subset=['label_binary'])\n",
    "\n",
    "# ถ้าต้องการ save เป็น CSV ใหม่\n",
    "#df.to_csv('news_dataset_truefakenews.csv', index=False)\n",
    "\n",
    "# ดูผลลัพธ์\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9b86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) ตัดคำ (ไม่เก็บช่องว่าง)\n",
    "def thai_token_list(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return word_tokenize(text, engine='newmm', keep_whitespace=False)\n",
    "\n",
    "df['title_tokens'] = df['title'].apply(thai_token_list)\n",
    "df['content_tokens'] = df['content'].apply(thai_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30430b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Infinix\\Documents\\GitHub\\Project_Thaifakenews\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Infinix\\.cache\\huggingface\\hub\\models--airesearch--wangchanberta-base-att-spm-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# 4) โหลดโมเดลภาษาไทย + จัด device\n",
    "MODEL_NAME = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e1f264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of tokenized titles:\n",
      "                                               title  \\\n",
      "0  OR ชวนร่วมเป็นเจ้าของกิจการ ลงทุนหุ้น IPO เริ่...   \n",
      "1  กระทรวงยุติธรรม เปิดเพจเฟซบุ๊กชื่อ Technology ...   \n",
      "2  หุ้น OKJ เปิดเทรดวันแรก ราคาเปิดพอร์ตเริ่มต้น ...   \n",
      "3  กรุงไทย เปิดให้จองสิทธิ์ยืมเงิน 100,000 บาท ลง...   \n",
      "4            ก.ล.ต. เปิดเว็บไซต์ให้ผู้ลงทุนสะสมคะแนน   \n",
      "\n",
      "                                        title_tokens  \n",
      "0  [OR, ชวน, ร่วม, เป็น, เจ้าของกิจการ, ลงทุน, หุ...  \n",
      "1  [กระทรวงยุติธรรม, เปิด, เพจ, เฟซบุ๊ก, ชื่อ, Te...  \n",
      "2  [หุ้น, OKJ, เปิด, เทรด, วัน, แรก, ราคา, เปิด, ...  \n",
      "3  [กรุงไทย, เปิด, ให้, จอง, สิทธิ์, ยืม, เงิน, 1...  \n",
      "4  [ก.ล.ต., เปิด, เว็บไซต์, ให้, ผู้ลงทุน, สะสม, ...  \n",
      "\n",
      "Sample of tokenized content:\n",
      "                                             content  \\\n",
      "0  จากที่มีการโฆษณาระบุเปิดให้ลงทุนหุ้น IPO ข้างต...   \n",
      "1  ระวังเพจปลอมแอบอ้างหน่วยงาน เพจเฟซบุ๊กชื่อ Tec...   \n",
      "2  เช็กข้อมูลให้ชัวร์ก่อนติดตาม เพจ “OH KAD” เป็น...   \n",
      "3  การปล่อยสินเชื่อ บนบัญชี TikTok ชื่อ ktb.thai ...   \n",
      "4  ดูให้ดี เช็กให้ชัวร์ เนื่องจากมิจฉาชีพได้นำโลโ...   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [จาก, ที่, มี, การ, โฆษณา, ระบุ, เปิด, ให้, ลง...  \n",
      "1  [ระวัง, เพจ, ปลอม, แอบอ้าง, หน่วยงาน, เพจ, เฟซ...  \n",
      "2  [เช็ก, ข้อมูล, ให้, ชัวร์, ก่อน, ติดตาม, เพจ, ...  \n",
      "3  [การ, ปล่อย, สินเชื่อ, บน, บัญชี, TikTok, ชื่อ...  \n",
      "4  [ดู, ให้, ดี, เช็ก, ให้, ชัวร์, เนื่องจาก, มิจ...  \n",
      "\n",
      "Token statistics:\n",
      "Average tokens in titles: 12.05\n",
      "Average tokens in content: 127.30\n"
     ]
    }
   ],
   "source": [
    "# Show sample of tokenized data\n",
    "print(\"Sample of tokenized titles:\")\n",
    "print(df[['title', 'title_tokens']].head())\n",
    "print(\"\\nSample of tokenized content:\")\n",
    "print(df[['content', 'content_tokens']].head())\n",
    "\n",
    "# Print token statistics\n",
    "print(\"\\nToken statistics:\")\n",
    "print(f\"Average tokens in titles: {df['title_tokens'].str.len().mean():.2f}\")\n",
    "print(f\"Average tokens in content: {df['content_tokens'].str.len().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "084ab25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# โหลด tokenizer และ model ก่อน\n",
    "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"  # ถ้าเป็นภาษาไทยอาจใช้ \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 5) ฟังก์ชัน embedding\n",
    "@torch.no_grad()\n",
    "def get_bert_embedding_from_tokens(tokens, max_length=128, use_mean_pool=False):\n",
    "    text = ' '.join(tokens)\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    outputs = model(**inputs)  # last_hidden_state: [1, L, H]\n",
    "    if use_mean_pool:\n",
    "        attn = inputs['attention_mask'].unsqueeze(-1)  # [1, L, 1]\n",
    "        summed = (outputs.last_hidden_state * attn).sum(dim=1)\n",
    "        denom = attn.sum(dim=1).clamp(min=1)\n",
    "        emb = (summed / denom).squeeze(0).cpu().numpy()\n",
    "    else:\n",
    "        emb = outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()  # CLS\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c758e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) สร้าง embedding\n",
    "use_content = True\n",
    "title_emb = np.vstack([get_bert_embedding_from_tokens(toks) for toks in df['title_tokens']])\n",
    "if use_content:\n",
    "    content_emb = np.vstack([get_bert_embedding_from_tokens(toks) for toks in df['content_tokens']])\n",
    "    x_np = np.concatenate([title_emb, content_emb], axis=1)\n",
    "else:\n",
    "    x_np = title_emb\n",
    "x_np = normalize(x_np, norm='l2', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977a52a",
   "metadata": {},
   "source": [
    "เสร็จ embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86784df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'ข่าวจริง': 0, 'ข่าวปลอม': 1}\n"
     ]
    }
   ],
   "source": [
    "# 7) แปลง labels -> ตัวเลข\n",
    "classes = sorted(df['label'].unique())\n",
    "label2id = {c:i for i,c in enumerate(classes)}\n",
    "y_np = df['label'].map(label2id).values\n",
    "print(\"Classes:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e9093f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) สร้าง kNN-graph\n",
    "N = x_np.shape[0]\n",
    "k = min(8, max(3, int(np.round(np.log2(max(N, 2))))))\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(x_np)\n",
    "dist, idx = nbrs.kneighbors(x_np)\n",
    "\n",
    "src = np.repeat(np.arange(N), k)\n",
    "dst = idx[:, 1:].reshape(-1)\n",
    "w = (1.0 - dist[:, 1:].reshape(-1))\n",
    "\n",
    "edge_index_np = np.vstack([src, dst])\n",
    "edge_weight_np = w\n",
    "\n",
    "# ทำ undirected + coalesce\n",
    "edge_index_np = np.concatenate([edge_index_np, edge_index_np[::-1, :]], axis=1)\n",
    "edge_weight_np = np.concatenate([edge_weight_np, edge_weight_np], axis=0)\n",
    "\n",
    "edge_index = torch.tensor(edge_index_np, dtype=torch.long)\n",
    "edge_weight = torch.tensor(edge_weight_np, dtype=torch.float)\n",
    "\n",
    "# ไม่มี m, n แล้ว\n",
    "edge_index, edge_weight = coalesce(edge_index, edge_weight, reduce='mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c99cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x_np.shape[0]\n",
    "k = min(8, max(3, int(np.round(np.log2(max(N, 2))))))  # สูตรง่ายสำหรับ N < 1000\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(x_np)\n",
    "dist, idx = nbrs.kneighbors(x_np)\n",
    "\n",
    "src = np.repeat(np.arange(N), k)\n",
    "dst = idx[:, 1:].reshape(-1)\n",
    "w = (1.0 - dist[:, 1:].reshape(-1))  # cosine similarity ในช่วง [0,1]\n",
    "\n",
    "edge_index_np = np.vstack([src, dst])\n",
    "edge_weight_np = w\n",
    "\n",
    "edge_index_np_rev = edge_index_np[::-1, :]\n",
    "edge_index_np = np.concatenate([edge_index_np, edge_index_np_rev], axis=1)\n",
    "edge_weight_np = np.concatenate([edge_weight_np, edge_weight_np], axis=0)\n",
    "\n",
    "edge_index = torch.tensor(edge_index_np, dtype=torch.long)\n",
    "edge_weight = torch.tensor(edge_weight_np, dtype=torch.float)\n",
    "\n",
    "# เวอร์ชันใหม่ของ PyG ไม่ต้องใช้ m, n\n",
    "edge_index, edge_weight = coalesce(edge_index, edge_weight, reduce='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57a5ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) สร้าง Data + split\n",
    "x = torch.tensor(x_np, dtype=torch.float)\n",
    "y = torch.tensor(y_np, dtype=torch.long)\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "data.edge_weight = edge_weight\n",
    "\n",
    "# split 70/15/15 stratified\n",
    "idx_all = np.arange(N)\n",
    "idx_train, idx_test = train_test_split(idx_all, test_size=0.15, stratify=y_np, random_state=42)\n",
    "idx_train, idx_val = train_test_split(idx_train, test_size=0.1765, stratify=y_np[idx_train], random_state=42)\n",
    "\n",
    "train_mask = torch.zeros(N, dtype=torch.bool); train_mask[idx_train] = True\n",
    "val_mask = torch.zeros(N, dtype=torch.bool); val_mask[idx_val] = True\n",
    "test_mask = torch.zeros(N, dtype=torch.bool); test_mask[idx_test] = True\n",
    "data.train_mask, data.val_mask, data.test_mask = train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95022cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) สร้าง GCN model\n",
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim, normalize=True)\n",
    "        self.conv2 = GCNConv(hid_dim, out_dim, normalize=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, ei, ew = data.x, data.edge_index, data.edge_weight\n",
    "        x = self.conv1(x, ei, edge_weight=ew)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, ei, edge_weight=ew)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3046ad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | loss 0.2657 | val_acc 0.6562 | val_f1 0.5883\n",
      "Epoch 040 | loss 0.2185 | val_acc 0.6562 | val_f1 0.6102\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# 11) Train GCN\n",
    "device_t = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_gnn = GCNNet(in_dim=x.shape[1], hid_dim=128, out_dim=len(classes), dropout=0.5).to(device_t)\n",
    "data = data.to(device_t)\n",
    "\n",
    "cls_w = compute_class_weight('balanced', classes=np.arange(len(classes)), y=y_np)\n",
    "cls_w = torch.tensor(cls_w, dtype=torch.float, device=device_t)\n",
    "\n",
    "opt = torch.optim.Adam(model_gnn.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "def evaluate(logits, mask):\n",
    "    y_true = data.y[mask].cpu().numpy()\n",
    "    y_pred = logits[mask].argmax(dim=1).cpu().numpy()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    return acc, f1m\n",
    "\n",
    "best_val_f1, best_state, patience, counter = -1, None, 30, 0\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    model_gnn.train()\n",
    "    opt.zero_grad()\n",
    "    out = model_gnn(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask], weight=cls_w)\n",
    "    loss.backward(); opt.step()\n",
    "\n",
    "    model_gnn.eval()\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_f1 = evaluate(out, data.val_mask)\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_state = {k: v.detach().cpu().clone() for k,v in model_gnn.state_dict().items()}\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | loss {loss.item():.4f} | val_acc {val_acc:.4f} | val_f1 {val_f1:.4f}\")\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40e32a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "if best_state is not None:\n",
    "    model_gnn.load_state_dict({k: v.to(device_t) for k,v in best_state.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a6d5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.8750 | Test F1-macro: 0.8333\n"
     ]
    }
   ],
   "source": [
    "# evaluate test set\n",
    "model_gnn.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model_gnn(data)\n",
    "    test_acc, test_f1 = evaluate(logits, data.test_mask)\n",
    "    print(f\"Test Acc: {test_acc:.4f} | Test F1-macro: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd3355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
