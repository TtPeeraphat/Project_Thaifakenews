{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3c3c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "from pythainlp.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad5382",
   "metadata": {},
   "source": [
    "pip install --quiet transformers\n",
    "pip install torch\n",
    "pip install pandas\n",
    "pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7bab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# โหลดไฟล์ CSV (กรณีชื่อคอลัมน์ 'label')\n",
    "df = pd.read_csv('dataset_truefakenews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de19749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# กำหนด mapping dictionary\n",
    "label_map = {\n",
    "    'ข่าวจริง': 'true',\n",
    "    'คลังความรู้': 'true',\n",
    "    'ข่าวปลอม': 'fake',\n",
    "    'อาชญากรออนไลน์': 'fake',\n",
    "    'ข่าวบิดเบือน': 'fake',\n",
    "    'ข่าวอื่นๆ': None  # หรือจะลบออกเลย\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03853ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date                                         link  \\\n",
      "0  23/07/2568 10:00:39  https://www.antifakenewscenter.com/?p=74610   \n",
      "1  23/07/2568 09:00:20  https://www.antifakenewscenter.com/?p=74581   \n",
      "2  23/07/2568 08:00:51  https://www.antifakenewscenter.com/?p=74577   \n",
      "3  23/07/2568 07:00:51  https://www.antifakenewscenter.com/?p=74573   \n",
      "4  23/07/2568 06:30:23  https://www.antifakenewscenter.com/?p=74569   \n",
      "\n",
      "                                               title  \\\n",
      "0  กระทรวงกลาโหม จัดประชุม GBC เซ็นสัญญายอมให้ทหา...   \n",
      "1  OR ชวนร่วมเป็นเจ้าของกิจการ ลงทุนหุ้น IPO เริ่...   \n",
      "2  กระทรวงยุติธรรม เปิดเพจเฟซบุ๊กชื่อ Technology ...   \n",
      "3  หุ้น OKJ เปิดเทรดวันแรก ราคาเปิดพอร์ตเริ่มต้น ...   \n",
      "4  กรุงไทย เปิดให้จองสิทธิ์ยืมเงิน 100,000 บาท ลง...   \n",
      "\n",
      "                                             content  \\\n",
      "0  ก่อนแชร์ต่อ อย่าลืมเช็ก ตามที่กองบัญชาการกองทั...   \n",
      "1  จากที่มีการโฆษณาระบุเปิดให้ลงทุนหุ้น IPO ข้างต...   \n",
      "2  ระวังเพจปลอมแอบอ้างหน่วยงาน เพจเฟซบุ๊กชื่อ Tec...   \n",
      "3  เช็กข้อมูลให้ชัวร์ก่อนติดตาม เพจ “OH KAD” เป็น...   \n",
      "4  การปล่อยสินเชื่อ บนบัญชี TikTok ชื่อ ktb.thai ...   \n",
      "\n",
      "                                              author         label  \\\n",
      "0                 กองบัญชาการกองทัพไทย กระทรวงกลาโหม  ข่าวบิดเบือน   \n",
      "1  บริษัท ปตท. น้ำมันและการค้าปลีก จำกัด (มหาชน) ...      ข่าวปลอม   \n",
      "2        สำนักงานปลัดกระทรวงยุติธรรม กระทรวงยุติธรรม      ข่าวปลอม   \n",
      "3                        ตลาดหลักทรัพย์แห่งประเทศไทย      ข่าวปลอม   \n",
      "4                       ธนาคารกรุงไทย กระทรวงการคลัง      ข่าวปลอม   \n",
      "\n",
      "               category  \n",
      "0  ความสงบและความมั่นคง  \n",
      "1          การเงิน-หุ้น  \n",
      "2  นโยบายรัฐบาล-ข่าวสาร  \n",
      "3          การเงิน-หุ้น  \n",
      "4          การเงิน-หุ้น  \n"
     ]
    }
   ],
   "source": [
    "# สร้างคอลัมน์ใหม่ 'label_binary' เป็น binary label จาก mapping\n",
    "#df['label_binary'] = df['label'].map(label_map)\n",
    "\n",
    "# ลบแถวที่เป็น None (ข่าวอื่นๆ)\n",
    "#df = df.dropna(subset=['label_binary'])\n",
    "\n",
    "# ถ้าต้องการ save เป็น CSV ใหม่\n",
    "#df.to_csv('news_dataset_truefakenews.csv', index=False)\n",
    "\n",
    "# ดูผลลัพธ์\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30430b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monsoon-nlp/bert-base-thai were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load Thai BERT model and tokenizer from monsoon-nlp\n",
    "model_name = \"monsoon-nlp/bert-base-thai\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Add device configuration and move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f6051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thai_token_list(text):\n",
    "    if pd.isna(text): return []\n",
    "    return word_tokenize(text, engine='newmm')\n",
    "\n",
    "df['title_tokens'] = df['title'].apply(thai_token_list)\n",
    "df['content_tokens'] = df['content'].apply(thai_token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e1f264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of tokenized titles:\n",
      "                                               title  \\\n",
      "0  กระทรวงกลาโหม จัดประชุม GBC เซ็นสัญญายอมให้ทหา...   \n",
      "1  OR ชวนร่วมเป็นเจ้าของกิจการ ลงทุนหุ้น IPO เริ่...   \n",
      "2  กระทรวงยุติธรรม เปิดเพจเฟซบุ๊กชื่อ Technology ...   \n",
      "3  หุ้น OKJ เปิดเทรดวันแรก ราคาเปิดพอร์ตเริ่มต้น ...   \n",
      "4  กรุงไทย เปิดให้จองสิทธิ์ยืมเงิน 100,000 บาท ลง...   \n",
      "\n",
      "                                        title_tokens  \n",
      "0  [กระทรวงกลาโหม,  , จัด, ประชุม,  , GBC,  , เซ็...  \n",
      "1  [OR,  , ชวน, ร่วม, เป็น, เจ้าของกิจการ,  , ลงท...  \n",
      "2  [กระทรวงยุติธรรม,  , เปิด, เพจ, เฟซบุ๊ก, ชื่อ,...  \n",
      "3  [หุ้น,  , OKJ,  , เปิด, เทรด, วัน, แรก,  , ราค...  \n",
      "4  [กรุงไทย,  , เปิด, ให้, จอง, สิทธิ์, ยืม, เงิน...  \n",
      "\n",
      "Sample of tokenized content:\n",
      "                                             content  \\\n",
      "0  ก่อนแชร์ต่อ อย่าลืมเช็ก ตามที่กองบัญชาการกองทั...   \n",
      "1  จากที่มีการโฆษณาระบุเปิดให้ลงทุนหุ้น IPO ข้างต...   \n",
      "2  ระวังเพจปลอมแอบอ้างหน่วยงาน เพจเฟซบุ๊กชื่อ Tec...   \n",
      "3  เช็กข้อมูลให้ชัวร์ก่อนติดตาม เพจ “OH KAD” เป็น...   \n",
      "4  การปล่อยสินเชื่อ บนบัญชี TikTok ชื่อ ktb.thai ...   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [ก่อน, แชร์, ต่อ,  , อย่า, ลืม, เช็ก,  , ตามที...  \n",
      "1  [จาก, ที่, มี, การ, โฆษณา, ระบุ, เปิด, ให้, ลง...  \n",
      "2  [ระวัง, เพจ, ปลอม, แอบอ้าง, หน่วยงาน,  , เพจ, ...  \n",
      "3  [เช็ก, ข้อมูล, ให้, ชัวร์, ก่อน, ติดตาม,  , เพ...  \n",
      "4  [การ, ปล่อย, สินเชื่อ,  , บน, บัญชี,  , TikTok...  \n",
      "\n",
      "Token statistics:\n",
      "Average tokens in titles: 15.04\n",
      "Average tokens in content: 176.71\n"
     ]
    }
   ],
   "source": [
    "# Show sample of tokenized data\n",
    "print(\"Sample of tokenized titles:\")\n",
    "print(df[['title', 'title_tokens']].head())\n",
    "print(\"\\nSample of tokenized content:\")\n",
    "print(df[['content', 'content_tokens']].head())\n",
    "\n",
    "# Print token statistics\n",
    "print(\"\\nToken statistics:\")\n",
    "print(f\"Average tokens in titles: {df['title_tokens'].str.len().mean():.2f}\")\n",
    "print(f\"Average tokens in content: {df['content_tokens'].str.len().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "084ab25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(word_list):\n",
    "    # รวมให้เป็นหนึ่ง string (แบ่งโดย space)\n",
    "    text = ' '.join(word_list)\n",
    "    # encode เป็น tensor เพื่อเข้า BERT\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,   # ค่า default, ปรับยาว-สั้นตาม input\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # ใช้ [CLS] (token ตัวแรกของ sequence) เป็นค่าแทนข้อความ\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c83f5475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tt_pe\\Documents\\GitHub\\Project_Thaifakenews\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\tt_pe\\Documents\\GitHub\\Project_Thaifakenews\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df['title_emb'] = df['title_tokens'].apply(get_bert_embedding)\n",
    "df['content_emb'] = df['content_tokens'].apply(get_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ade1b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(df['title_emb'].iloc[0].shape)  # ควรได้ (768,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977a52a",
   "metadata": {},
   "source": [
    "เสร็จ embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
