{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3c3c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "from pythainlp.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad5382",
   "metadata": {},
   "source": [
    "pip install --quiet transformers\n",
    "pip install torch\n",
    "pip install pandas\n",
    "pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb7bab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# โหลดไฟล์ CSV (กรณีชื่อคอลัมน์ 'label')\n",
    "df = pd.read_csv('dataset_truefakenews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de19749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# กำหนด mapping dictionary\n",
    "label_map = {\n",
    "    'ข่าวจริง': 'true',\n",
    "    'คลังความรู้': 'true',\n",
    "    'ข่าวปลอม': 'fake',\n",
    "    'อาชญากรออนไลน์': 'fake',\n",
    "    'ข่าวบิดเบือน': 'fake',\n",
    "    'ข่าวอื่นๆ': None  # หรือจะลบออกเลย\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03853ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date                                         link  \\\n",
      "0  23/07/2568 10:00:39  https://www.antifakenewscenter.com/?p=74610   \n",
      "1  23/07/2568 09:00:20  https://www.antifakenewscenter.com/?p=74581   \n",
      "2  23/07/2568 08:00:51  https://www.antifakenewscenter.com/?p=74577   \n",
      "3  23/07/2568 07:00:51  https://www.antifakenewscenter.com/?p=74573   \n",
      "4  23/07/2568 06:30:23  https://www.antifakenewscenter.com/?p=74569   \n",
      "\n",
      "                                               title  \\\n",
      "0  กระทรวงกลาโหม จัดประชุม GBC เซ็นสัญญายอมให้ทหา...   \n",
      "1  OR ชวนร่วมเป็นเจ้าของกิจการ ลงทุนหุ้น IPO เริ่...   \n",
      "2  กระทรวงยุติธรรม เปิดเพจเฟซบุ๊กชื่อ Technology ...   \n",
      "3  หุ้น OKJ เปิดเทรดวันแรก ราคาเปิดพอร์ตเริ่มต้น ...   \n",
      "4  กรุงไทย เปิดให้จองสิทธิ์ยืมเงิน 100,000 บาท ลง...   \n",
      "\n",
      "                                             content  \\\n",
      "0  ก่อนแชร์ต่อ อย่าลืมเช็ก ตามที่กองบัญชาการกองทั...   \n",
      "1  จากที่มีการโฆษณาระบุเปิดให้ลงทุนหุ้น IPO ข้างต...   \n",
      "2  ระวังเพจปลอมแอบอ้างหน่วยงาน เพจเฟซบุ๊กชื่อ Tec...   \n",
      "3  เช็กข้อมูลให้ชัวร์ก่อนติดตาม เพจ “OH KAD” เป็น...   \n",
      "4  การปล่อยสินเชื่อ บนบัญชี TikTok ชื่อ ktb.thai ...   \n",
      "\n",
      "                                              author         label  \\\n",
      "0                 กองบัญชาการกองทัพไทย กระทรวงกลาโหม  ข่าวบิดเบือน   \n",
      "1  บริษัท ปตท. น้ำมันและการค้าปลีก จำกัด (มหาชน) ...      ข่าวปลอม   \n",
      "2        สำนักงานปลัดกระทรวงยุติธรรม กระทรวงยุติธรรม      ข่าวปลอม   \n",
      "3                        ตลาดหลักทรัพย์แห่งประเทศไทย      ข่าวปลอม   \n",
      "4                       ธนาคารกรุงไทย กระทรวงการคลัง      ข่าวปลอม   \n",
      "\n",
      "               category  \n",
      "0  ความสงบและความมั่นคง  \n",
      "1          การเงิน-หุ้น  \n",
      "2  นโยบายรัฐบาล-ข่าวสาร  \n",
      "3          การเงิน-หุ้น  \n",
      "4          การเงิน-หุ้น  \n"
     ]
    }
   ],
   "source": [
    "# สร้างคอลัมน์ใหม่ 'label_binary' เป็น binary label จาก mapping\n",
    "#df['label_binary'] = df['label'].map(label_map)\n",
    "\n",
    "# ลบแถวที่เป็น None (ข่าวอื่นๆ)\n",
    "#df = df.dropna(subset=['label_binary'])\n",
    "\n",
    "# ถ้าต้องการ save เป็น CSV ใหม่\n",
    "#df.to_csv('news_dataset_truefakenews.csv', index=False)\n",
    "\n",
    "# ดูผลลัพธ์\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30430b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monsoon-nlp/bert-base-thai were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load Thai BERT model and tokenizer from monsoon-nlp\n",
    "model_name = \"monsoon-nlp/bert-base-thai\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Add device configuration and move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8f6051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thai_token_list(text):\n",
    "    if pd.isna(text): return []\n",
    "    return word_tokenize(text, engine='newmm')\n",
    "\n",
    "df['title_tokens'] = df['title'].apply(thai_token_list)\n",
    "df['content_tokens'] = df['content'].apply(thai_token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e1f264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of tokenized titles:\n",
      "                                               title  \\\n",
      "0  กระทรวงกลาโหม จัดประชุม GBC เซ็นสัญญายอมให้ทหา...   \n",
      "1  OR ชวนร่วมเป็นเจ้าของกิจการ ลงทุนหุ้น IPO เริ่...   \n",
      "2  กระทรวงยุติธรรม เปิดเพจเฟซบุ๊กชื่อ Technology ...   \n",
      "3  หุ้น OKJ เปิดเทรดวันแรก ราคาเปิดพอร์ตเริ่มต้น ...   \n",
      "4  กรุงไทย เปิดให้จองสิทธิ์ยืมเงิน 100,000 บาท ลง...   \n",
      "\n",
      "                                        title_tokens  \n",
      "0  [กระทรวงกลาโหม,  , จัด, ประชุม,  , GBC,  , เซ็...  \n",
      "1  [OR,  , ชวน, ร่วม, เป็น, เจ้าของกิจการ,  , ลงท...  \n",
      "2  [กระทรวงยุติธรรม,  , เปิด, เพจ, เฟซบุ๊ก, ชื่อ,...  \n",
      "3  [หุ้น,  , OKJ,  , เปิด, เทรด, วัน, แรก,  , ราค...  \n",
      "4  [กรุงไทย,  , เปิด, ให้, จอง, สิทธิ์, ยืม, เงิน...  \n",
      "\n",
      "Sample of tokenized content:\n",
      "                                             content  \\\n",
      "0  ก่อนแชร์ต่อ อย่าลืมเช็ก ตามที่กองบัญชาการกองทั...   \n",
      "1  จากที่มีการโฆษณาระบุเปิดให้ลงทุนหุ้น IPO ข้างต...   \n",
      "2  ระวังเพจปลอมแอบอ้างหน่วยงาน เพจเฟซบุ๊กชื่อ Tec...   \n",
      "3  เช็กข้อมูลให้ชัวร์ก่อนติดตาม เพจ “OH KAD” เป็น...   \n",
      "4  การปล่อยสินเชื่อ บนบัญชี TikTok ชื่อ ktb.thai ...   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [ก่อน, แชร์, ต่อ,  , อย่า, ลืม, เช็ก,  , ตามที...  \n",
      "1  [จาก, ที่, มี, การ, โฆษณา, ระบุ, เปิด, ให้, ลง...  \n",
      "2  [ระวัง, เพจ, ปลอม, แอบอ้าง, หน่วยงาน,  , เพจ, ...  \n",
      "3  [เช็ก, ข้อมูล, ให้, ชัวร์, ก่อน, ติดตาม,  , เพ...  \n",
      "4  [การ, ปล่อย, สินเชื่อ,  , บน, บัญชี,  , TikTok...  \n",
      "\n",
      "Token statistics:\n",
      "Average tokens in titles: 15.04\n",
      "Average tokens in content: 176.71\n"
     ]
    }
   ],
   "source": [
    "# Show sample of tokenized data\n",
    "print(\"Sample of tokenized titles:\")\n",
    "print(df[['title', 'title_tokens']].head())\n",
    "print(\"\\nSample of tokenized content:\")\n",
    "print(df[['content', 'content_tokens']].head())\n",
    "\n",
    "# Print token statistics\n",
    "print(\"\\nToken statistics:\")\n",
    "print(f\"Average tokens in titles: {df['title_tokens'].str.len().mean():.2f}\")\n",
    "print(f\"Average tokens in content: {df['content_tokens'].str.len().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbf1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "084ab25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# โหลด tokenizer และ model ก่อน\n",
    "model_name = \"bert-base-multilingual-cased\"  # ถ้าเป็นภาษาไทยอาจใช้ \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embedding(word_list):\n",
    "    # รวมคำเป็น string\n",
    "    text = ' '.join(word_list)\n",
    "    # encode เป็น tensor\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # ใช้ [CLS] token เป็น embedding\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c83f5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_emb'] = df['title_tokens'].apply(get_bert_embedding)\n",
    "df['content_emb'] = df['content_tokens'].apply(get_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ade1b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(df['title_emb'].iloc[0].shape)  # ควรได้ (768,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977a52a",
   "metadata": {},
   "source": [
    "เสร็จ embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e9093f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'กิจกรรม': 0, 'ข่าวจริง': 1, 'ข่าวจริิง': 2, 'ข่าวบิดเบือน': 3, 'ข่าวปลอม': 4, 'ข่าวอื่นๆ': 5, 'ข่่าวจริง': 6, 'ข่่าวบิดเบือน': 7, 'คลังความรู้': 8, 'อาชญากรรมออนไลน์': 9}\n"
     ]
    }
   ],
   "source": [
    "use_content = True\n",
    "model.eval()\n",
    "\n",
    "title_emb = np.vstack([get_bert_embedding(w) for w in df['title_tokens']])\n",
    "\n",
    "if use_content: content_emb = np.vstack([get_bert_embedding(w) for w in df['content_tokens']]) \n",
    "x_np = np.concatenate([title_emb, content_emb], axis=1) # concat title+content else: x_np = title_emb\n",
    "x_np = normalize(x_np, norm='l2', axis=1)\n",
    "labels_str = df['label'].astype(str) \n",
    "classes = sorted(labels_str.unique()) \n",
    "label2id = {c: i for i, c in enumerate(classes)} \n",
    "y_np = labels_str.map(label2id).values\n",
    "\n",
    "print(\"Classes:\", label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a62526",
   "metadata": {},
   "source": [
    "สร้างกราฟ kNN + edge weight (cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c99cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x_np.shape[0] \n",
    "k = min(8, max(3, int(np.round(np.log2(max(N, 2))))))\n",
    " # สูตรง่ายสำหรับ N < 1000 nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(x_np) dist, idx = nbrs.kneighbors(x_np)\n",
    "src = np.repeat(np.arange(N), k) \n",
    "dst = idx[:, 1:].reshape(-1) w = (1.0 - dist[:, 1:].reshape(-1)) # cosine similarity ในช่วง [0,1]\n",
    "edge_index_np = np.vstack([src, dst]) \n",
    "edge_weight_np = w\n",
    "edge_index_np_rev = edge_index_np[::-1, :] \n",
    "edge_index_np = np.concatenate([edge_index_np, edge_index_np_rev], axis=1) \n",
    "edge_weight_np = np.concatenate([edge_weight_np, edge_weight_np], axis=0)\n",
    "edge_index = torch.tensor(edge_index_np, dtype=torch.long) \n",
    "edge_weight = torch.tensor(edge_weight_np, dtype=torch.float) \n",
    "edge_index, edge_weight = coalesce(edge_index, edge_weight, m=N, n=N, reduce='mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
