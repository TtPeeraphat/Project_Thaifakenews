import streamlit as st
import torch
import numpy as np
import pickle
from torch_geometric.data import Data
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import normalize 
from collections import Counter
from transformers import AutoTokenizer, AutoModel
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
import re

# ==========================================
# 1. SETUP & LOAD DATA
# ==========================================
st.set_page_config(page_title="AI ‡∏Ç‡πà‡∏≤‡∏ß‡∏õ‡∏•‡∏≠‡∏° Detector", page_icon="üïµÔ∏è")

@st.cache_resource
def load_resources():
    # A) ‡πÇ‡∏´‡∏•‡∏î BERT
    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
    lm_model = AutoModel.from_pretrained('xlm-roberta-base')
    
    # B) ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö
    with open('artifacts.pkl', 'rb') as f:
        artifacts = pickle.load(f)
    
    # C) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (KNN)
    nbrs = NearestNeighbors(n_neighbors=artifacts['k'], metric='cosine')
    nbrs.fit(artifacts['x_np'])
    
    return tokenizer, lm_model, artifacts, nbrs

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î
tokenizer, lm_model, artifacts, nbrs = load_resources()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ==========================================
# 2. DEFINE MODEL CLASS
# ==========================================
class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, data):
        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr
        x = self.conv1(x, edge_index, edge_weight=edge_attr)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index, edge_weight=edge_attr)
        return x

# ==========================================
# 3. LOAD TRAINED MODEL
# ==========================================
model = GCN(in_channels=768, hidden_channels=256, out_channels=2) 
model.load_state_dict(torch.load('best_model.pth', map_location=device))
model.to(device)
model.eval()

# ==========================================
# 4. PREDICTION FUNCTION
# ==========================================

def get_bert_embeddings_batch(texts, tokenizer, model, device, max_length=256, batch_size=32, use_mean_pool=True):
    model.eval()
    all_embeddings = []
    for start_idx in range(0, len(texts), batch_size):
        batch_texts = texts[start_idx:start_idx + batch_size]
        batch_texts = ["" if (isinstance(t, float) and np.isnan(t)) else str(t) for t in batch_texts]
        
        inputs = tokenizer(batch_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt').to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            last_hidden = outputs.last_hidden_state
            
            if use_mean_pool:
                attn = inputs['attention_mask'].unsqueeze(-1)
                summed = (last_hidden * attn).sum(dim=1)
                denom = attn.sum(dim=1).clamp(min=1)
                emb = (summed / denom).cpu().numpy()
            else:
                emb = last_hidden[:, 0, :].cpu().numpy()
        all_embeddings.append(emb)
    return np.vstack(all_embeddings)
def clean_text(text):
    # ‡∏•‡∏ö URL
    text = re.sub(r'http\S+', '', text)
    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏© (‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà ‡πÑ‡∏ó‡∏¢ ‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)
    text = re.sub(r'[^a-zA-Z0-9\u0E00-\u0E7F\s]', '', text)
    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÜ
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def predict(content):
    content = clean_text(content)
    # 1. Embed (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö‡πÜ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Normalize)
    emb = get_bert_embeddings_batch([content], tokenizer, lm_model, device, max_length=256, use_mean_pool=True)
    emb_vec = emb[0] 
    
    # ‚ö†Ô∏è ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÉ‡∏ä‡πâ vector ‡∏î‡∏¥‡∏ö‡πÜ ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Normalize ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Training Data
    emb_norm = normalize(emb_vec.reshape(1, -1), axis=1, norm='l2')[0]
    
    # 2. KNN Search
    dists, idxs = nbrs.kneighbors(emb_norm.reshape(1, -1))
    idxs = idxs[0]
    
    # 3. Logic ‡∏´‡∏≤‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà
    neighbor_cats = [artifacts['id2cat'][artifacts['y_cat_np'][i]] for i in idxs]
    most_common_cat = Counter(neighbor_cats).most_common(1)[0][0]
    
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á Graph
    topn = artifacts['k']
    X_new = np.vstack([emb_norm, artifacts['x_np'][idxs]]) # ‡∏£‡∏ß‡∏°‡πÇ‡∏´‡∏ô‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô
    
    center = 0
    neighbors = np.arange(1, topn + 1)
    edge_index = np.concatenate([
        np.stack([np.full(topn, center), neighbors]),
        np.stack([neighbors, np.full(topn, center)])
    ], axis=1)
    
    edge_attr = np.concatenate([1 - dists[0], 1 - dists[0]])
    
    data_new = Data(
        x=torch.tensor(X_new, dtype=torch.float, device=device),
        edge_index=torch.tensor(edge_index, dtype=torch.long, device=device),
        edge_attr=torch.tensor(edge_attr, dtype=torch.float, device=device),
        batch=torch.zeros(X_new.shape[0], dtype=torch.long, device=device)
    )
    
    # 5. ‡πÄ‡∏Ç‡πâ‡∏≤ Model
    with torch.no_grad():
        logits = model(data_new)
        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()
        pred_id = int(np.argmax(probs))
    
    return {
        'label': artifacts['id2label'][pred_id],
        'prob': probs[pred_id],
        'category': most_common_cat,
        'neighbors': neighbor_cats,
        'pred_id': pred_id,
        'raw_probs': probs.tolist()
    }

# ==========================================
# 5. UI (‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö)
# ==========================================
st.title("üïµÔ∏è Fake News Detection System")
st.write("‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏õ‡∏•‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ GNN + BERT")

news_text = st.text_area("‡∏ß‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:", height=150)

if st.button("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"):
    if not news_text:
        st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö")
    else:
        with st.spinner('AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå...'):
            try:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠
                result = predict(news_text)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                st.divider()
                col1, col2 = st.columns(2)
                
                with col1:
                    if result['label'] == '‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏£‡∏¥‡∏á': 
                        st.success(f"## ‚úÖ {result['label']}")
                    else:
                        st.error(f"## üö® {result['label']}")
                    st.metric("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à (Confidence)", f"{result['prob']*100:.2f}%")
                
                with col2:
                    st.info(f"**‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà:** {result['category']}")
                    st.write("**‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á:**")
                    st.write(result['neighbors'])
                    
                # ‡∏™‡πà‡∏ß‡∏ô Debug
                with st.expander("üõ†Ô∏è Debug Information"):
                    st.write(f"**Predicted ID:** {result['pred_id']}")
                    st.write(f"**Label Mapping:** {artifacts['id2label']}")
                    st.write(f"**Raw Probabilities:** {result['raw_probs']}")
                    
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")